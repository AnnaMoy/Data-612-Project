---
title: "Data 612 - Project 4"
author: "Anna Moy & Natalie Kalukeerthie"
date: "2025-06-23"
output: html_document
---

```{r, message= FALSE, warning=FALSE}
# Load libraries
library(recommenderlab)
library(Matrix)
```


The Jester 5k dataset contains 5k users and 100 jokes and the ratings ranges from -10 to 10. 

We load the built- in Jester 5k data set from the recommederlab package to create a basic recommender system. 

```{r}
# Load the built-in Jester5k dataset (5,000 users × 100 jokes)
data(Jester5k, package = "recommenderlab")
rrm <- Jester5k
```

Using evaluation Scheme, we split data into training (80%) and testing (20%) sets.given = 10 is used here so each user keeps 10 known ratings for training and the rest is "unknown: and used for test predictions. 

```{r}
# 80–20 split, 10 observed ratings per user retained for "known"
scheme <- evaluationScheme(rrm, method = "split", train = 0.8, given = 10, goodRating = 3)

```

We train two models which is the user-user collaborative filtering (UBCF) and Singluar Value Decomposition (SVD).

User based collaborative filtering (UBCF) - recommends items liked by user with similar tastes
Singular Value Decomposition (SVD) - matrix factorization that finds hidden patterns in user preferences

```{r}
# User-User Collaborative Filtering (UBCF)
rec_UBCF <- Recommender(getData(scheme, "train"), method = "UBCF")

# SVD-based model
rec_SVD <- Recommender(getData(scheme, "train"), method = "SVD")

```

Predicts the rating for both the UBCF and SVD and compared the Root Mean Square Error (RSME), Mean Absolute Error (MAE) and Mean Squared Error (MSE) to determine which one has a better prediction. 

```{r}
# Predict ratings
pred_UBCF <- predict(rec_UBCF, getData(scheme, "known"), type = "ratings")
pred_SVD  <- predict(rec_SVD,  getData(scheme, "known"), type = "ratings")

# Evaluate accuracy (RMSE/MAE)
acc <- rbind(
  UBCF = calcPredictionAccuracy(pred_UBCF, getData(scheme, "unknown")),
  SVD  = calcPredictionAccuracy(pred_SVD,  getData(scheme, "unknown"))
)
print(acc)

```

Overall the results of Singular Value Decomposition (SVD) outperforms the User based collaborative filtering as the RMSE, MSE and MAE was the lowest which means it has a better accuracy in its predictions. RMSE measures the average error which is 4.513169 and MAE indicates on average the predictions were off by 3.539866 points. 

```{r, warning = FALSE}
# Top-N from original SVD model
topN_SVD <- predict(rec_SVD, getData(scheme, "known"), type = "topNList", n = 5)
original_items <- as(topN_SVD, "list")

# Shuffle top-N recommendations to simulate a more diverse rerank
set.seed(123)
diverse_list <- lapply(original_items, function(x) sample(x, length(x)))
diverse_list <- lapply(diverse_list, as.integer)

# Rebuild new topNList object
pred_diverse <- new("topNList",
                    items = diverse_list,
                    itemLabels = colnames(rrm),
                    n = as.integer(5))



```

```{r}
# Evaluate both versions using evaluate()
eval_orig <- evaluate(scheme, method = "SVD", type = "topNList", n = 5)

# For Diverse version (manually evaluated)
# First define function to compute diversity between users
list_diversity <- function(topNlist) {
  lists <- as(topNlist, "list")
  all_items <- unique(unlist(lists))
  avg_diversity <- mean(sapply(1:length(lists), function(i) {
    others <- lists[-i]
    overlap <- mean(sapply(others, function(x) length(intersect(x, lists[[i]]))/length(union(x, lists[[i]]))))
    return(1 - overlap)
  }))
  return(avg_diversity)
}

# Diversity comparison
diversity_orig <- list_diversity(topN_SVD)
diversity_diverse <- list_diversity(pred_diverse)

cat(sprintf("Original Diversity: %.3f\nDiverse Diversity: %.3f\n", diversity_orig, diversity_diverse))
```

```{r, warning = FALSE}
# Baseline predictions (ratings)
pred_SVD <- predict(rec_SVD, getData(scheme, "known"), type = "ratings")

# Accuracy of baseline predictions
acc_baseline <- calcPredictionAccuracy(pred_SVD, getData(scheme, "unknown"))

# Original top-N list from SVD
topN_SVD <- predict(rec_SVD, getData(scheme, "known"), type = "topNList", n = 5)

# Calculate baseline diversity
diversity_orig <- list_diversity(topN_SVD)

cat(sprintf("Baseline RMSE: %.3f, MAE: %.3f, MSE: %.3f\n", acc_baseline["RMSE"], acc_baseline["MAE"], acc_baseline["MSE"]))
cat(sprintf("Baseline Diversity: %.3f\n", diversity_orig))


# Shuffle recommendations to increase diversity
set.seed(123)
diverse_list <- lapply(as(topN_SVD, "list"), function(x) sample(x, length(x)))
diverse_list <- lapply(diverse_list, as.integer)

# Create new topNList for diverse recommendations
pred_diverse <- new("topNList",
                    items = diverse_list,
                    itemLabels = colnames(rrm),
                    n = as.integer(5))

# Calculate diversity after re-ranking
diversity_diverse <- list_diversity(pred_diverse)

cat(sprintf("Diverse Diversity: %.3f\n", diversity_diverse))


# Evaluate original SVD top-N recommendations
eval_orig <- evaluate(scheme, method = "SVD", type = "topNList", n = 5)

# Since diverse list is custom, calculate precision/recall manually or
# you can approximate by measuring overlap with true test data

# Print summary of original evaluation
avg_precision_orig <- avg(eval_orig, "prec")
avg_recall_orig <- avg(eval_orig, "rec")

cat(sprintf("Original Precision: %.3f, Recall: %.3f\n", avg_precision_orig, avg_recall_orig))

# For diverse list, manual evaluation needed — typically precision might drop slightly because of less popular items

```
